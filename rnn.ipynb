{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from convRNN import ConvRNN\n",
    "from conv3DRNN import Conv3DRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import convRNN\n",
    "\n",
    "import importlib\n",
    "importlib.reload(convRNN)\n",
    "\n",
    "depth, height, width = 128, 128, 128\n",
    "path_size = 16 # Generating random 3D data for CT, PTV, OAR, and a target dose\n",
    "# depth, height, width = 32, 32, 32\n",
    "# Generating random data\n",
    "ct = torch.rand(1, 1, depth, height, width, device=device)  # 1 batch, 1 channel, depth, height, width\n",
    "ptv = torch.rand(1, 1, depth, height, width, device=device)\n",
    "oar = torch.rand(1, 1, depth, height, width, device=device)\n",
    "target_dose = torch.rand(1, 1, depth, height, width, device=device)  # This is what the model will learn to predict\n",
    "\n",
    "combined_input = torch.cat((ct, ptv, oar), dim=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "times to downsample:  4\n",
      "z3 shape:  torch.Size([1, 96, 8, 8, 1])\n",
      "z6 shape:  torch.Size([1, 96, 8, 8, 1])\n",
      "z9 shape:  torch.Size([1, 96, 8, 8, 1])\n",
      "z3 shape:  torch.Size([1, 96, 8, 8, 1])\n",
      "z6 shape:  torch.Size([1, 96, 8, 8, 1])\n",
      "z9 shape:  torch.Size([1, 96, 8, 8, 1])\n",
      "x shape:  torch.Size([1, 8, 12, 8, 8, 1])\n",
      "shape of output3:  torch.Size([1, 4, 64, 64, 64])\n",
      "shape of output6:  torch.Size([1, 8, 32, 32, 32])\n",
      "shape of output9:  torch.Size([1, 16, 16, 16, 16])\n",
      "\n",
      "upsample z9 shape:  torch.Size([1, 8, 32, 32, 32])\n",
      "\n",
      "\n",
      "upsample z6 shape:  torch.Size([1, 4, 64, 64, 64])\n",
      "\n",
      "upsample z3 shape:  torch.Size([1, 2, 128, 128, 128])\n",
      "feature map shape:  torch.Size([1, 2, 128, 128, 128])\n",
      "\n",
      "output shape:  torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import unet3dconv_version\n",
    "\n",
    "import importlib\n",
    "importlib.reload(unet3dconv_version)\n",
    "\n",
    "unetr = unet3dconv_version.UNET3Dconv(img_shape=(depth, height, width), input_dim=3, output_dim=1, patch_size=path_size).to(device)\n",
    "output = unetr(combined_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map shape:  torch.Size([1, 128, 3, 128, 128, 1])\n",
      "h0:  torch.Size([1, 96, 128, 128, 1])\n",
      "output shape:  torch.Size([1, 128, 96, 128, 128, 1])\n",
      "torch.Size([1, 96, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import conv3DRNN\n",
    "\n",
    "import importlib\n",
    "importlib.reload(conv3DRNN)\n",
    "\n",
    "\n",
    "unetr = conv3DRNN.TransformerConv3DRNN(input_dim=3, \n",
    "                                       output_dim=1,\n",
    "                                       input_channel_size_rnn=3,\n",
    "                                       ).to(device)\n",
    "output = unetr(combined_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  torch.Size([1, 128, 256, 12, 16])\n",
      "h0 shape:  torch.Size([1, 96, 12, 16])\n",
      "output shape:  torch.Size([1, 128, 96, 12, 16])\n",
      "output shape:  torch.Size([1, 3, 128, 128, 48])\n",
      "output shape:  torch.Size([1, 3, 128, 128, 128])\n",
      "torch.Size([1, 3, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "unetr = convRNN.TransformerConvRNN(input_dim=3, output_dim=1).to(device)\n",
    "\n",
    "output = unetr(combined_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize the ConvRNN model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mConvRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize hidden state with the correct shape\u001b[39;00m\n\u001b[1;32m     19\u001b[0m h_0 \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mzeros(B, hidden_channels, H, W)]\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/DL2/convRNN.py:248\u001b[0m, in \u001b[0;36mConvRNN.__init__\u001b[0;34m(self, input_channels, hidden_channels, kernel_size, num_layers)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_channels: \u001b[38;5;28mint\u001b[39m, hidden_channels: \u001b[38;5;28mint\u001b[39m, kernel_size: \u001b[38;5;28mint\u001b[39m, num_layers: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    Initialize a ConvRNN.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m        num_layers (int): Number of ConvRNN layers.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mConvRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m=\u001b[39m num_layers\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_channels \u001b[38;5;241m=\u001b[39m hidden_channels\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "B = 1  # Batch size\n",
    "T = 10  # Sequence length (number of time steps)\n",
    "C_in = 3  # Number of input channels\n",
    "H = 12  # Height\n",
    "W = 16  # Width\n",
    "\n",
    "x = torch.randn(B, T, C_in, H, W)  # Example input tensor\n",
    "\n",
    "# Define ConvRNN parameters\n",
    "input_channels = C_in\n",
    "hidden_channels = 96\n",
    "kernel_size = 3\n",
    "num_layers = 1\n",
    "\n",
    "# Initialize the ConvRNN model\n",
    "model = ConvRNN(input_channels, hidden_channels, kernel_size, num_layers)\n",
    "\n",
    "# Initialize hidden state with the correct shape\n",
    "h_0 = [torch.zeros(B, hidden_channels, H, W)]\n",
    "\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Hidden state shape: \", [h.shape for h in h_0])\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states = model(x, h_0)\n",
    "print(\"Output shape: \", outputs.shape)\n",
    "print(\"Hidden state shape: \", [h.shape for h in hidden_states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h shape: torch.Size([1, 1, 2304])\n",
      "Sequence length: 128\n",
      "x shape: torch.Size([1, 128, 3, 128, 128])\n",
      "torch.Size([1, 128, 5, 128, 128])\n",
      "torch.Size([1, 5, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# feature_map, z3, z6, z9 = unetr(combined_input)\n",
    "\n",
    "# # concatentate z3, z6, z9\n",
    "# z = torch.cat((z3, z6, z9), dim=2) # shape: [1, 96, 12, 4, 4])\n",
    "\n",
    "# # flatten\n",
    "# h = z.view(1, 1, -1)\n",
    "# print(f\"h shape: {h.shape}\")\n",
    "\n",
    "# step_size_input = 1\n",
    "# seq_len = combined_input.shape[2] // step_size_input\n",
    "# print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "# x = [combined_input[:, :, i:i+step_size_input, ...] for i in range(0, combined_input.shape[2], step_size_input)]\n",
    "# x = torch.stack(x, dim=1) # we now have batch, seq_len, channel, 1, height, width\n",
    "# x = x.squeeze(3)\n",
    "# print(f\"x shape: {x.shape}\")\n",
    "# batch, seq_len, channel, height, width = x.shape\n",
    "# # # flatten the last 3 dims\n",
    "# # x = x.view(batch, seq_len, channel * step_size_input * height * width)\n",
    "\n",
    "# # print(f\"encoded states shape: {x.shape}\")\n",
    "# input_channels = channel\n",
    "# hidden_channels = 5\n",
    "# hidden_size = h.shape[2]\n",
    "# num_layers = 1\n",
    "# kernel_size = 3\n",
    "\n",
    "# conv_rnn = ConvRNN(input_channels, hidden_channels, kernel_size, num_layers).to(device)\n",
    "\n",
    "# # h with shape (batch_size, self.hidden_channels, height, width)\n",
    "\n",
    "# outputs, hidden_states = conv_rnn(x)\n",
    "# print(outputs.shape)\n",
    "# print(hidden_states[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "# from torch.nn import RNN\n",
    "\n",
    "# rnn = RNN(input_size, hidden_size, num_layers, batch_first=True).to(device)\n",
    "\n",
    "# output, h_n = rnn(x, h)\n",
    "\n",
    "# print(f\"RNN output shape: {output.shape}\")\n",
    "# print(f\"RNN h_n shape: {h_n.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/thijm/GitHub/Master_related/DL2/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h shape: torch.Size([1, 1, 2304])\n",
      "Sequence length: 16\n",
      "encoded states shape: torch.Size([1, 16, 393216])\n",
      "RNN output shape: torch.Size([1, 16, 2304])\n",
      "RNN h_n shape: torch.Size([1, 1, 2304])\n"
     ]
    }
   ],
   "source": [
    "feature_map, z3, z6, z9 = unetr(combined_input)\n",
    "\n",
    "# concatentate z3, z6, z9\n",
    "z = torch.cat((z3, z6, z9), dim=2) # shape: [1, 96, 12, 4, 4])\n",
    "\n",
    "# flatten\n",
    "h = z.view(1, 1, -1)\n",
    "print(f\"h shape: {h.shape}\")\n",
    "\n",
    "step_size_input = 8\n",
    "seq_len = combined_input.shape[2] // step_size_input\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "x = [combined_input[:, :, i:i+step_size_input, ...] for i in range(0, combined_input.shape[2], step_size_input)]\n",
    "x = torch.stack(x, dim=1) # we now have batch, seq_len, channel, step_size_input, height, width\n",
    "\n",
    "batch, seq_len, channel, step_size_input, height, width = x.shape\n",
    "\n",
    "# flatten the last 3 dims\n",
    "x = x.view(batch, seq_len, channel * step_size_input * height * width)\n",
    "print(f\"encoded states shape: {x.shape}\")\n",
    "\n",
    "\n",
    "input_size = x.shape[2]\n",
    "# hidden_size = z.shape[1]\n",
    "hidden_size = h.shape[2]\n",
    "num_layers = 1\n",
    "\n",
    "from torch.nn import RNN\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, num_layers, batch_first=True).to(device)\n",
    "\n",
    "output, h_n = rnn(x, h)\n",
    "\n",
    "print(f\"RNN output shape: {output.shape}\")\n",
    "print(f\"RNN h_n shape: {h_n.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "#         super(RNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Set initial hidden and cell states \n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "#         # Forward propagate LSTM\n",
    "#         out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "    \n",
    "    \n",
    "# import torch rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalMGRU(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.SourceAxisDistance = config.source_axis\n",
    "        #self.res_y = config.res_axis\n",
    "        #self.NH = config.n_hidden\n",
    "\n",
    "        self.SourceAxisDistance = 100.0\n",
    "        self.res_y = 0.25\n",
    "        self.NH = 64\n",
    "\n",
    "        n_kernel = 3\n",
    "        self.NY = 6\n",
    "        self.NX = 4\n",
    "\n",
    "        self.convXR = nn.Conv2d(self.NX, self.NH, n_kernel, padding='same', padding_mode='replicate')\n",
    "        self.convXZ = nn.Conv2d(self.NX, self.NH, n_kernel, padding='same', padding_mode='replicate')\n",
    "        self.convXN = nn.Conv2d(self.NX, self.NH, n_kernel, padding='same', padding_mode='replicate')\n",
    "\n",
    "        self.linYR = nn.Linear(self.NY, self.NH)\n",
    "        self.linYZ = nn.Linear(self.NY, self.NH)\n",
    "        self.linYN = nn.Linear(self.NY, self.NH)\n",
    "\n",
    "        self.convHR = nn.Conv2d(self.NH, self.NH, n_kernel, padding='same', padding_mode='replicate')\n",
    "        self.convHZ = nn.Conv2d(self.NH, self.NH, n_kernel, padding='same', padding_mode='replicate')\n",
    "        self.convHN = nn.Conv2d(self.NH, self.NH, n_kernel, padding='same', padding_mode='replicate')\n",
    "\n",
    "        self.linearOut = nn.Linear(self.NH, 8)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, gantry, Fluence, Grid, Slices_CT, ymin):\n",
    "        dimB = Grid.size(0)\n",
    "        dimX = Grid.size(2)\n",
    "        dimY = len(Slices_CT)\n",
    "        dimZ = Grid.size(1)\n",
    "        #device0 = Segment.device\n",
    "        #collimatorAngle = Segment[:, 0]\n",
    "        #gantryAngle = Segment[:, 2:4]\n",
    "        device0 = \"cuda:0\"\n",
    "        nb=2\n",
    "        #collimatorAngle = torch.full((nb,), 20.0, device=device0, dtype=torch.float64)\n",
    "        #gantryAngle = torch.full((nb,), gantry, device=device0, dtype=torch.float64)\n",
    "        collimatorAngle = torch.full((nb,), 20.0, device=device0)\n",
    "        gantryAngle = torch.full((nb,), gantry, device=device0)\n",
    "\n",
    "        #print(Grid.permute(0, 3, 1, 2).shape)   -> ([4, 2, 23, 23])\n",
    "        print(Fluence.unsqueeze(1).shape)\n",
    "\n",
    "        XI_ = torch.cat((Grid.permute(0, 3, 1, 2), Fluence.unsqueeze(1)), dim=1)\n",
    "        '''\n",
    "        YI_ = torch.stack(\n",
    "            (collimatorAngle.deg2rad().sin(), collimatorAngle.deg2rad().cos(),\n",
    "             (gantryAngle[:, 1] - gantryAngle[:, 0]).deg2rad().sin(),\n",
    "             (gantryAngle[:, 1] - gantryAngle[:, 0]).deg2rad().cos()\n",
    "             ), dim=-1)\n",
    "        '''\n",
    "\n",
    "        YI_ = torch.stack(\n",
    "            (collimatorAngle.deg2rad().sin(), collimatorAngle.deg2rad().cos(),\n",
    "             (gantryAngle).deg2rad().sin(),\n",
    "             (gantryAngle).deg2rad().cos()\n",
    "             ), dim=-1)\n",
    "\n",
    "        h0 = torch.zeros((1, 1, 1, 1), dtype=torch.float32, device=device0).expand(dimB, self.NH, dimZ, dimX)\n",
    "\n",
    "        out = []\n",
    "        for y in range(dimY):\n",
    "            ### conic effect\n",
    "            ycoor = (ymin + y) * self.res_y\n",
    "            xz_fac = (self.SourceAxisDistance + ycoor) / self.SourceAxisDistance\n",
    "            ### conic effect\n",
    "\n",
    "            XI = torch.cat((Slices_CT[y].unsqueeze(1), XI_), dim=1)\n",
    "            YI = torch.cat(\n",
    "                (YI_, torch.tensor((ycoor, xz_fac), dtype=torch.float32, device=device0).unsqueeze(0).expand(dimB, -1)),\n",
    "                dim=-1)\n",
    "\n",
    "            r = self.sigmoid(self.convXR(XI) + self.linYR(YI).unsqueeze(-1).unsqueeze(-1) + self.convHR(h0))\n",
    "            z = self.sigmoid(self.convXZ(XI) + self.linYZ(YI).unsqueeze(-1).unsqueeze(-1) + self.convHZ(h0))\n",
    "            n = torch.tanh(self.convXN(XI) + self.linYN(YI).unsqueeze(-1).unsqueeze(-1) + r * self.convHN(h0))\n",
    "\n",
    "            ht = (1 - z) * n + z * h0\n",
    "\n",
    "            out.append(ht)\n",
    "\n",
    "            h0 = ht\n",
    "\n",
    "        return self.linearOut(torch.stack(out, dim=3).permute(0, 2, 3, 4, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
