#!/bin/bash
#SBATCH -J parallel_unetr_training
#SBATCH -t 1:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus=4
#SBATCH --cpus-per-task=18
#SBATCH --output=slurm_output_%A.out


module purge
module load 2022
module load CUDA/11.8.0
module load Python/3.10.4-GCCcore-11.3.0

# copy dataset over
cp -r "$HOME/.cache/huggingface" "$TMPDIR/data_cache"
cp -r "$HOME/DL2" "$TMPDIR/work"

export HF_DATASETS_CACHE="$TMPDIR/data_cache"

cd "$TMPDIR/work"
source .venv/bin/activate


srun python -m src.main --dry-run --parallel --epochs 200 --batch-size 16
