#!/bin/bash
#SBATCH --partition=gpu
#SBATCH -J parallel_unetr_training
#SBATCH -t 1:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus=1
#SBATCH --cpus-per-task=18
#SBATCH --output=slurm_output_%A.out
#SBATCH --error=slurm_error_%A.err


module purge
module load 2022
module load CUDA/11.8.0
module load Python/3.10.4-GCCcore-11.3.0

mkdir -p "$TMPDIR/work"

# copy dataset over
rsync -r "$HOME/DL2" "$TMPDIR/work" --exclude .venv --exclude .git --exclude .checkpoints --exclude wandb

export HF_DATASETS_CACHE="$TMPDIR/data_cache"

cd "$TMPDIR/work/DL2"

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

srun python -u -m src.main --epochs 200 --batch-size 4
