#!/bin/bash
#SBATCH --partition=gpu
#SBATCH -J parallel_unetr_training
#SBATCH -t 1:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus=2
#SBATCH --cpus-per-task=18
#SBATCH --output=slurm_output_%A.out


module purge
module load 2022
module load CUDA/11.8.0
module load Python/3.10.4-GCCcore-11.3.0

# copy dataset over
rsync "$HOME/DL2" "$TMPDIR/work" --exclude .venv --exclude .git --exclude .checkpoints --exclude wandb

export HF_DATASETS_CACHE="$TMPDIR/data_cache"

cd "$TMPDIR/work"

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

srun python -m src.main --parallel --epochs 200 --batch-size 4
